{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WddUyUssWuel"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZJVKjUBRYHS4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzVwAv51YXTA",
        "outputId": "f452f005-185c-41bd-dce5-7ca9974b5bde"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CRqgBs22YXYX"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnJCwz2sYXbX",
        "outputId": "7416c2a7-2353-452d-a016-b74d02ddb267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.13.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: typing-extensions in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from SpeechRecognition) (4.12.2)\n",
            "Downloading SpeechRecognition-3.13.0-py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.13.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "!pip install SpeechRecognition\n",
        "import speech_recognition as sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww2nJW7bYXdU",
        "outputId": "cda09153-dcdf-4165-dde6-f69eb58a047a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from gTTS) (2.32.3)\n",
            "Collecting click<8.2,>=7.1 (from gTTS)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from requests<3,>=2.27->gTTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from requests<3,>=2.27->gTTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from requests<3,>=2.27->gTTS) (2024.12.14)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Installing collected packages: click, gTTS\n",
            "Successfully installed click-8.1.8 gTTS-2.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install gTTS\n",
        "\n",
        "from gtts import gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSwwSpwnZ4Pn",
        "outputId": "d864f976-2742-4889-ef08-063b7aecc682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from requests->youtube-transcript-api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from requests->youtube-transcript-api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/haarithkhan/miniconda3/envs/hk/lib/python3.12/site-packages (from requests->youtube-transcript-api) (2024.12.14)\n",
            "Downloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api\n",
        "import os\n",
        "from youtube_transcript_api import YouTubeTranscriptApi # This line should work after installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3jmOcYq4Z4R_"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/week2.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the file_path variable before using it\u001b[39;00m\n\u001b[1;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/week2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace your_file.json with the actual path to your file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
            "File \u001b[0;32m~/miniconda3/envs/hk/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/week2.json'"
          ]
        }
      ],
      "source": [
        "# Define the file_path variable before using it\n",
        "file_path = \"/content/week2.json\"  # Replace your_file.json with the actual path to your file\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaQxo9xZZ4U0"
      },
      "outputs": [],
      "source": [
        " import pandas as pd\n",
        "\n",
        "def create_dataset(data):  # Assuming you want to create a function to process the data\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for item in data['data']:\n",
        "        for paragraph in item['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                answer = qa['answers'][0]  # Assuming one answer per question\n",
        "                contexts.append(context)\n",
        "                questions.append(question)\n",
        "                answers.append({\"text\": [answer['text']], \"answer_start\": [answer['answer_start']]})\n",
        "    # Indentation corrected here\n",
        "    df = pd.DataFrame({\"context\": contexts, \"question\": questions, \"answers\": answers})\n",
        "\n",
        "    # Assumed you want to import and use Dataset (likely from datasets library)\n",
        "    from datasets import Dataset\n",
        "    return Dataset.from_pandas(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XltjWd2rZ4WP"
      },
      "outputs": [],
      "source": [
        "import json # Import the json module to access json.load()\n",
        "\n",
        "file_path = r\"/content/week2.json\"  # Replace with your JSON file path\n",
        "\n",
        "# Use json.load() to read the JSON data from the file\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ-FNNEDZ4Y_"
      },
      "outputs": [],
      "source": [
        "model_name = \"bert-base-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrv78OJpZ4d3",
        "outputId": "ecc7bf26-5d87-42a9-c560-51b08611265e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "eb78a9aa777a40128930307350fc9be6",
            "4633ad5cb0004e0aad0928439ea5503c",
            "2bc4ca1af51f42f9892ad14d4110b63b",
            "43141cbbefb846b391d6d4314903d722",
            "34b82fe227ab46c0b30c01ae818cba86",
            "d8d0cb4d79ef49a294f837aa3b3fba08",
            "23bb83d7a7454364be050dcc66a7ad88",
            "f55389ad675342059c57ce0add52384a",
            "c4ca716bdae84b1a9bb901ca0ad31b4c",
            "f09a3c402185437a822e3c957a8b80c3",
            "0d5d73d4705942f1b30717cd5112e88c"
          ]
        },
        "id": "KuabklD6ohH9",
        "outputId": "2ce652bf-8884-4cf6-ab5c-f8fd7670ea10"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb78a9aa777a40128930307350fc9be6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load data from JSON file\n",
        "file_path = r\"/content/week2.json\"  # Replace with your JSON file path\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Create Dataset object using the function you already defined\n",
        "dataset = create_dataset(data)\n",
        "\n",
        "# ... (Rest of your code)\n",
        "\n",
        "# Load tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "# Apply the preprocessing to the Dataset\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVGKe-SloZMl",
        "outputId": "df659feb-7a9d-4152-b3b1-c0cad35e767f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsZmAt1FoZQF"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Trainer, TrainingArguments, AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "# ... (Your previous code to load and preprocess the dataset) ...\n",
        "\n",
        "# Convert the Hugging Face Dataset to NumPy arrays for the specific columns\n",
        "# Assuming you want to convert the 'input_ids', 'attention_mask', and 'token_type_ids' columns:\n",
        "dataset_np = {\n",
        "    'input_ids': np.array(tokenized_dataset['input_ids']),\n",
        "    'attention_mask': np.array(tokenized_dataset['attention_mask']),\n",
        "    'token_type_ids': np.array(tokenized_dataset['token_type_ids'])  # Add if this column exists\n",
        "}\n",
        "\n",
        "# Split the NumPy arrays into train and evaluation sets\n",
        "train_indices, eval_indices = train_test_split(np.arange(len(dataset_np['input_ids'])), test_size=0.2, random_state=42)  # Adjust test_size as needed\n",
        "\n",
        "# Create Hugging Face Datasets from the split indices \n",
        "train_dataset = tokenized_dataset.select(train_indices)\n",
        "eval_dataset = tokenized_dataset.select(eval_indices)\n",
        "\n",
        "# ... (Rest of your training code) ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFIiXMnuqzw9",
        "outputId": "441e4fb5-2a2d-4604-c02d-22fdd4ac7b5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./fine_tuned_model/tokenizer_config.json',\n",
              " './fine_tuned_model/special_tokens_map.json',\n",
              " './fine_tuned_model/vocab.txt',\n",
              " './fine_tuned_model/added_tokens.json',\n",
              " './fine_tuned_model/tokenizer.json')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMNkN8WGqz53"
      },
      "outputs": [],
      "source": [
        "def speech_to_text():\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Listening for your question...\")\n",
        "        audio = recognizer.listen(source)\n",
        "    try:\n",
        "        question = recognizer.recognize_google(audio)\n",
        "        print(f\"You asked: {question}\")\n",
        "        return question\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Sorry, I could not understand the audio.\")\n",
        "        return None\n",
        "\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"response.mp3\")\n",
        "    os.system(\"start response.mp3\")  # Use 'start' for Windows; 'open' for MacOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpsUMXogrB2j"
      },
      "outputs": [],
      "source": [
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(\n",
        "        tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
        "    )\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1eGbVSgqz9c"
      },
      "outputs": [],
      "source": [
        "def get_transcript(video_id):\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        transcript_text = \" \".join([entry['text'] for entry in transcript])\n",
        "        return transcript_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching transcript: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT4BubKqsOCW",
        "outputId": "8fbdfa97-2611-47c8-9037-34b256496cf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "portaudio19-dev is already the newest version (19.6.0-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n",
            "Requirement already satisfied: PyAudio in /usr/local/lib/python3.10/dist-packages (0.2.14)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install portaudio19-dev\n",
        "!pip install PyAudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owkMf1pdtEZx",
        "outputId": "2a7c169c-acc2-40f6-dfb5-933fd105c430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_D17e86teRk",
        "outputId": "3a28c918-a812-40a7-f998-3db3ebb53d19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Files removed: 2\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip  # Upgrade pip\n",
        "!pip cache purge           # Clear pip cache\n",
        "!pip install SpeechRecognition --index-url https://pypi.org/simple/ # Install with specific index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgiGgYltu-IJ",
        "outputId": "db8709a3-c746-4b08-c0a4-afdd79bb3bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "alsa-utils is already the newest version (1.2.6-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install alsa-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER1n3Bj8vRWD"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "import pyaudio\n",
        "\n",
        "def speech_to_text():\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    # List available audio devices\n",
        "    print(\"Available audio devices:\")\n",
        "    p = pyaudio.PyAudio()\n",
        "    for i in range(p.get_device_count()):\n",
        "        device_info = p.get_device_info_by_index(i)\n",
        "        print(f\"{i}: {device_info['name']}\")\n",
        "    p.terminate()\n",
        "\n",
        "    # Allow user to select audio device\n",
        "    device_index = int(input(\"Select audio device index: \"))\n",
        "\n",
        "    with sr.Microphone(device_index=device_index) as source:\n",
        "        print(\"Listening for your question...\")\n",
        "        audio = recognizer.listen(source)\n",
        "    try:\n",
        "        question = recognizer.recognize_google(audio)\n",
        "        print(f\"You asked: {question}\")\n",
        "        return question\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Sorry, I could not understand the audio.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvOvxzGbt7sq",
        "outputId": "8d239d44-d503-4dae-9780-7807286ad088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arecord: device_list:274: no soundcards found...\n",
            "aplay: device_list:274: no soundcards found...\n"
          ]
        }
      ],
      "source": [
        "!arecord -l  # List available audio input devices\n",
        "!aplay -l   # List available audio output devices\n",
        "# If you don't see any input devices listed, you may need to install audio drivers or enable audio in system settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ6r32pNs6o6",
        "outputId": "668e2285-722c-4dbd-a197-8444c2e4080b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement speech_recognition (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for speech_recognition\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install speech_recognition PyAudio\n",
        "import speech_recognition as sr\n",
        "import pyaudio  # Import pyaudio explicitly\n",
        "\n",
        "def speech_to_text():\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    # Print available audio devices\n",
        "    print(\"Available audio devices:\")\n",
        "    p = pyaudio.PyAudio()\n",
        "    for i in range(p.get_device_count()):\n",
        "        device_info = p.get_device_info_by_index(i)\n",
        "        print(f\"{i}: {device_info['name']}\")\n",
        "    p.terminate()\n",
        "\n",
        "    with sr.Microphone() as source: # Initialize source, after printing audio devices\n",
        "        print(\"Listening for your question...\")\n",
        "        audio = recognizer.listen(source)\n",
        "    try:\n",
        "        question = recognizer.recognize_google(audio)\n",
        "        print(f\"You asked: {question}\")\n",
        "        return question\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Sorry, I could not understand the audio.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmu8CujByd8x"
      },
      "outputs": [],
      "source": [
        "import pyaudio\n",
        "\n",
        "p = pyaudio.PyAudio()\n",
        "info = p.get_host_api_info_by_index(0)\n",
        "numdevices = info.get('deviceCount')\n",
        "\n",
        "for i in range(0, numdevices):\n",
        "    if (p.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n",
        "        print(\"Input Device id \", i, \" - \", p.get_device_info_by_host_api_device_index(0, i).get('name'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "bPeR0D4OrLEU",
        "outputId": "968c4823-6dc7-483b-822c-7588325e398a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcript fetched successfully.\n",
            "Available audio devices:\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "No Default Input Device Available",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-b347372c57b5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transcript fetched successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeech_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use the STT to capture the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-28d1f69fe5de>\u001b[0m in \u001b[0;36mspeech_to_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMicrophone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Initialize source, after printing audio devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Listening for your question...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, device_index, sample_rate, chunk_size)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mdevice_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Device index out of range ({} devices available; device index should be between 0 and {} inclusive)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# automatically set the sample rate to the hardware's default sample rate if not specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mdevice_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_info_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_input_device_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"defaultSampleRate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"defaultSampleRate\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Invalid device info returned from PyAudio: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"defaultSampleRate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyaudio/__init__.py\u001b[0m in \u001b[0;36mget_default_input_device_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \"\"\"\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mdevice_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_input_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_info_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No Default Input Device Available"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\": # Changed _name_ to __name__\n",
        "    video_id = \"w8Dq8blTmSA\"  # Replace this with the actual YouTube video ID of NPTEL\n",
        "    context = get_transcript(video_id)  # Fetch the transcript from the NPTEL video\n",
        "\n",
        "    if context:\n",
        "        print(\"Transcript fetched successfully.\")\n",
        "        question = speech_to_text()  # Use the STT to capture the question\n",
        "\n",
        "        if question:\n",
        "            answer = answer_question(question, context)\n",
        "            print(f\"Answer: {answer}\")\n",
        "            text_to_speech(answer)  # Convert the answer to speech\n",
        "        else:\n",
        "            print(\"No question detected.\")\n",
        "    else:\n",
        "        print(\"Could not fetch transcript.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hk",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d5d73d4705942f1b30717cd5112e88c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23bb83d7a7454364be050dcc66a7ad88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bc4ca1af51f42f9892ad14d4110b63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f55389ad675342059c57ce0add52384a",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4ca716bdae84b1a9bb901ca0ad31b4c",
            "value": 10
          }
        },
        "34b82fe227ab46c0b30c01ae818cba86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43141cbbefb846b391d6d4314903d722": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f09a3c402185437a822e3c957a8b80c3",
            "placeholder": "​",
            "style": "IPY_MODEL_0d5d73d4705942f1b30717cd5112e88c",
            "value": " 10/10 [00:00&lt;00:00, 178.38 examples/s]"
          }
        },
        "4633ad5cb0004e0aad0928439ea5503c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8d0cb4d79ef49a294f837aa3b3fba08",
            "placeholder": "​",
            "style": "IPY_MODEL_23bb83d7a7454364be050dcc66a7ad88",
            "value": "Map: 100%"
          }
        },
        "c4ca716bdae84b1a9bb901ca0ad31b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8d0cb4d79ef49a294f837aa3b3fba08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb78a9aa777a40128930307350fc9be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4633ad5cb0004e0aad0928439ea5503c",
              "IPY_MODEL_2bc4ca1af51f42f9892ad14d4110b63b",
              "IPY_MODEL_43141cbbefb846b391d6d4314903d722"
            ],
            "layout": "IPY_MODEL_34b82fe227ab46c0b30c01ae818cba86"
          }
        },
        "f09a3c402185437a822e3c957a8b80c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f55389ad675342059c57ce0add52384a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
